# @package _global_
defaults:
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

# global
wandb_log: True
run_name: 'hash'
project_name: 'recon_pretraining'
seed: 42
cuda: True
tracking_metric: "psnr"
load_config: False
load_weights: False

hydra:
  job:
    chdir: True
  sweep:
    dir: "/projects/nef_recon/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}_${run_name}"
  run:
    dir: "outputs"

log_steps:
  volumes: 5000
  loss: 100
  metrics: 5000
  gradients: 5000

val_log_steps:
  volumes: 1000
  loss: 100
  metrics: 500

# neural fields
nef:
  type: 'Hash' # choice of SIREN, MLP, RFF, Hash
  num_layers: 3
  num_hidden: 256
  final_act: 'relu'
  hash:
    num_levels: 8
    level_dim: 2
    base_resolution: 16
    log2_max_params_per_level: 21
    skip_conn: False
  rff:
    std: 5.0
    learnable_coefficients: False
  SIREN:
    omega: 300.0
    learnable_omega: False
  conditioning:
    do: True
    type: 'neural_modulation_field'  # choice of neural_modulation_field, code, code_field
    warmup_steps: 0
    zero_init: True
    neural_modulation_field:
      type: 'Hash' # choice of SIREN, MLP, RFF, Hash
      num_layers: 2
      num_hidden: 64
      final_act: False
      hash:
        num_levels: 16
        level_dim: 2
        base_resolution: 4
        log2_max_params_per_level: 21
        skip_conn: False
      rff:
        std: 5.0
        learnable_coefficients: False
      SIREN:
        omega: 7.0
        learnable_omega: False
#    code:
#      code_dim: 1024
#      coord_embedding:
#        type: 'Hash' # choice of SIREN, MLP, RFF, Hash
#        num_layers: 2
#        num_hidden: 64
#        final_act: False
#        rff:
#          std: 5.0
#          learnable_coefficients: False
#        hash:
#          num_levels: 16
#          level_dim: 8
#          base_resolution: 16
#          log2_max_params_per_level: 21
#          skip_conn: False

# trainer
training:
  do: True
  lr: 1e-3
  lr_conditioning: 1e-4
  batch_size: 32000
  gradient_accumulation_steps: 1
  epochs: 1000
  per_patient_batching: True # To ensure all sampled points are from the same patient
  num_workers: 8
  checkpoint_steps: 500 # number of steps for checkpointing
  lr_step_size: 20000 # number of steps for lr decay
  lr_gamma: 0.9 # lr decay factor
  max_time: 6000000 # in seconds
  validate_every: 10000  # frequency of validation in number of steps
  shuffle: True
  lr_patience: 150 # number of iterations of patience for the reduce on plateau scheduler

# optimizer
optimizer:
  nef:
    type: 'adam'
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.0
  conditioning:
    type: 'sgd'
    momentum: 0.0
    nesterov: False
    weight_decay: 0.0

# data
dataset:
  path: '../../../data/volumes_recon_pretraining' # path to dataset
  original_volumes_path: '../../../data/volumes' # path to original volumes
  num_vols: 200  # number of volumes to use
  name: 'reconstructions'

# validation
validation:
  do: True
  early_stopping_step_interval: 500
  patience: 3
  delta: .01
  lr: 1e-3
  batch_size: 1024
  epochs: 100
  num_workers: 16
  lr_step_size: 20000 # number of steps for lr decay
  lr_gamma: 0.9 # lr decay factor
  max_time: 300 # in seconds
  shuffle: True
  per_patient_batching: False
  max_iters: None
  max_epochs: 1000000

# optimizer
val_optimizer:
  type: 'adam'
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.0

# data
val_dataset:
  path: '../../../data/volumes' # path to dataset
  num_vols: 1  # number of volumes to use
  num_projs: 400
  num_steps: 300
  projs_sample_step: 8
  noisy_projections: False
  name: 'projections'
  volume_id: 0
  max_epochs: 2
  stage: 'val'
  max_iters: None
